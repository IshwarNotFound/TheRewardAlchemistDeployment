# -*- coding: utf-8 -*-
"""
app.py: Flask Web Application for GNN Campaign Recommender

This Flask application provides a web interface to interact with the pre-trained
GraphSAGE campaign recommendation model (trained using main.py).
It allows users to input their profile details via a web form,
loads the necessary model artifacts and graph data (downloading from the
ARTIFACT_URL specified in environment variables if needed), performs on-demand
GNN and SBERT embedding calculations, predicts relevant campaigns,
ranks them based on GNN score, proximity, and semantic similarity,
and displays the top recommendations with explanations.

Requires:
    - Flask, PyTorch, PyTorch Geometric, sentence-transformers, pandas, etc.
      (see requirements.txt)
    - Pre-trained artifacts generated by main.py located in the
      'preprocessed_data_gnn/' directory (or downloaded via ARTIFACT_URL):
        - recommender_gnn_artifacts_hackathon_final_v2.pkl
        - campaigns_preprocessed_gnn_hackathon_final_v2.csv
        - graph_data_hackathon_final_v2.pt
    - An HTML template at templates/index.html
"""

# === Imports ===
import os
import requests # For downloading
import zipfile  # For extracting ZIP files
import shutil   # For file operations (less needed now)
import pandas as pd
import numpy as np
import joblib
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics.pairwise import cosine_similarity
from datetime import datetime, timedelta
import logging
import torch
import torch.nn.functional as F
from torch.nn import Module, Linear, ReLU, Dropout
from flask import Flask, render_template, request
import collections # Keep this for defaultdict if used in data gen lists
import gc
import random
import time # Import time for potential sleep in debug
from sentence_transformers import SentenceTransformer
try:
    from torch_geometric.nn import SAGEConv
    from torch_geometric.data import Data
except ImportError as e:
    print(f"CRITICAL ERROR: torch-geometric library or dependencies not found: {e}")
    print("Please ensure torch-geometric, torch-scatter, torch-sparse are installed correctly.")
    exit(1) # Exit if core dependency is missing
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

# === Flask App Setup ===
app = Flask(__name__)

# === Configuration ===
logging.basicConfig(level=logging.INFO, format='%(asctime)s-%(levelname)s-[%(filename)s:%(lineno)d]-%(message)s', datefmt='%Y-%m-%d %H:%M:%S')
logger = logging.getLogger(__name__)

# --- File/Directory Paths ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
PREPROCESSED_DIR = os.path.join(BASE_DIR, "preprocessed_data_gnn") # Target directory for artifacts
MODEL_ARTIFACTS_FILE = "recommender_gnn_artifacts_hackathon_final_v2.pkl"
CAMPAIGNS_PREPROCESSED_FILE = "campaigns_preprocessed_gnn_hackathon_final_v2.csv"
GRAPH_DATA_FILE = "graph_data_hackathon_final_v2.pt"

# --- Column Names (Match Data Gen Script) ---
USER_ID_COL = 'user_id'; USER_AGE_COL = 'age_group'; USER_LOCATION_COL = 'location'; USER_LOCATION_TIER_COL = 'location_tier'; USER_DEVICE_COL = 'device_type'; USER_INTERESTS_COL = 'interests'; USER_ACTIVITY_COL = 'activity_level'; USER_MONETARY_COL = 'monetary_level'; USER_PAST_REWARDS_COL = 'past_rewards_list'; USER_WATCH_CATEGORIES_COL = 'recent_watch_categories'; USER_WEATHER_COL = 'simulated_weather'; USER_CREATION_DATE_COL = 'profile_creation_date'
CAMPAIGN_ID_COL = 'campaign_id'; CAMPAIGN_BUSINESS_NAME_COL = 'business_name'; CAMPAIGN_CATEGORY_COL = 'category'; CAMPAIGN_LOCATION_COL = 'location'; CAMPAIGN_PROMO_COL = 'promo'; CAMPAIGN_TARGET_GROUP_COL = 'target_group'; CAMPAIGN_BUDGET_COL = 'budget'; CAMPAIGN_TARGET_DEVICE_COL = 'target_device'

# --- Device Types & Location Maps (Match Data Gen Script) ---
DEVICE_TYPES = ["Android Smartphone (Mid-Range)", "Android Smartphone (High-End)", "iOS Smartphone", "Tablet (Android)", "Tablet (iOS)", "Windows Desktop/Laptop", "MacOS Desktop/Laptop"]
LOCATION_STANDARDIZATION_MAP = {"delhi (ncr)": "delhi", "new delhi": "delhi", "delhi": "delhi", "bangalore (bengaluru)": "bangalore", "bengaluru": "bangalore", "mumbai": "mumbai", "varanasi (benaras)": "varanasi", "benaras": "varanasi", "chennai": "chennai", "kolkata": "kolkata", "hyderabad": "hyderabad", "pune": "pune", "ahmedabad": "ahmedabad", "jaipur": "jaipur", "surat": "surat", "lucknow": "lucknow", "kanpur": "kanpur", "nagpur": "nagpur", "indore": "indore", "thane": "thane", "bhopal": "bhopal", "visakhapatnam": "visakhapatnam", "patna": "patna", "vadodara": "vadodara", "ludhiana": "ludhiana", "agra": "agra", "nashik": "nashik", "coimbatore": "coimbatore", "kochi": "kochi", "chandigarh": "chandigarh", "mysore": "mysore", "amritsar": "amritsar", "guwahati": "guwahati", "shimla": "shimla", "goa": "goa", "rishikesh": "rishikesh", "udaipur": "udaipur", "darjeeling": "darjeeling", "madurai": "madurai", "jodhpur": "jodhpur", "puducherry": "puducherry", "aurangabad": "aurangabad", "dehradun": "dehradun", "bhubaneswar": "bhubaneswar", "raipur": "raipur", "gurgaon": "gurugram", "gurugram": "gurugram", "unknown": "unknown", "nan": "unknown", "": "unknown"}
LOCATION_TIERS_MAP = {"mumbai": "Tier 1", "delhi": "Tier 1", "bangalore": "Tier 1", "chennai": "Tier 1", "kolkata": "Tier 1", "hyderabad": "Tier 1", "pune": "Tier 1", "ahmedabad": "Tier 1", "gurugram": "Tier 1", "noida": "Tier 1", "jaipur": "Tier 2", "surat": "Tier 2", "lucknow": "Tier 2", "kanpur": "Tier 2", "nagpur": "Tier 2", "indore": "Tier 2", "thane": "Tier 2", "bhopal": "Tier 2", "visakhapatnam": "Tier 2", "patna": "Tier 2", "vadodara": "Tier 2", "ludhiana": "Tier 2", "agra": "Tier 2", "nashik": "Tier 2", "coimbatore": "Tier 2", "kochi": "Tier 2", "chandigarh": "Tier 2", "mysore": "Tier 2", "varanasi": "Tier 3", "amritsar": "Tier 3", "guwahati": "Tier 3", "madurai": "Tier 3", "jodhpur": "Tier 3", "aurangabad": "Tier 3", "dehradun": "Tier 3", "bhubaneswar": "Tier 3", "raipur": "Tier 3", "shimla": "Specialty", "goa": "Specialty", "rishikesh": "Specialty", "udaipur": "Specialty", "darjeeling": "Specialty", "puducherry": "Specialty", "unknown": "Unknown"}
LOCATION_REGIONS_MAP = {"NCR": ["delhi", "gurugram", "noida", "faridabad", "ghaziabad"], "Mumbai MMR": ["mumbai", "thane", "navi mumbai"], "South Tier 1": ["bangalore", "chennai", "hyderabad"], "West Tier 1/2": ["pune", "ahmedabad", "surat", "vadodara", "indore", "nagpur", "nashik", "bhopal", "aurangabad"], "North Tier 2/3/S": ["jaipur", "lucknow", "kanpur", "ludhiana", "chandigarh", "agra", "amritsar", "shimla", "dehradun", "rishikesh", "jodhpur"], "East Tier 1/2/3/S": ["kolkata", "patna", "bhubaneswar", "guwahati", "darjeeling", "raipur"], "South Tier 2/3/S": ["visakhapatnam", "coimbatore", "kochi", "mysore", "madurai", "puducherry", "goa", "varanasi"], "Unknown Region": ["unknown"]}

# --- Options for Checkboxes (Derived from Data Gen Script) ---
INTERESTS_OPTIONS = sorted(["Cricket (IPL fanatic)", "Bollywood Masala", "Regional Cinema (Tamil/Telugu/etc.)", "Indian Politics", "Stock Market (Dalal Street)", "Travel (Domestic/International)", "Street Food Safari", "Indian Festivals & Culture", "Reality TV (Big Boss)", "Stand-up Comedy (Indian comics)", "Yoga & Meditation", "Classical Music/Dance", "Reading (Indian Authors)", "Mythology & History", "Ayurveda & Wellness", "Astrology & Vastu", "Traditional Crafts", "Gardening (Home/Organic)", "Technology & Gadgets", "Online Gaming (BGMI/FreeFire)", "Social Media (Influencers/Memes)", "Startup Ecosystem", "Fitness & Gym", "Sustainable Living", "Online Shopping (Fashion/Electronics)", "Learning New Skills/Languages", "DIY & Home Improvement", "Photography", "Cooking & Recipes (Regional/Global)", "Blogging/Vlogging", "Cars & Bikes", "Parenting", "Health & Nutrition"])
DEFAULT_WATCH_CATEGORIES_GEN = ["comedy sketches (indian creators)", "trending music videos (india)", "whatsapp status videos", "news bulletins", "devotional songs (bhajans)", "short films"]
INTEREST_TO_WATCH_MAP_GEN = collections.defaultdict(lambda: DEFAULT_WATCH_CATEGORIES_GEN); INTEREST_TO_WATCH_MAP_GEN['cricket (ipl fanatic)'] = ["ipl match highlights", "expert analysis shows", "fantasy league tips", "cricketer interviews", "live match streams"]; INTEREST_TO_WATCH_MAP_GEN['bollywood masala'] = ["latest movie trailers", "song releases", "celebrity gossip", "film award shows", "movie reviews", "behind the scenes"]; INTEREST_TO_WATCH_MAP_GEN['regional cinema (tamil/telugu/etc.)'] = ["regional movie trailers", "tollywood/kollywood news", "actor interviews", "audio launches", "regional movie streams"]; INTEREST_TO_WATCH_MAP_GEN['indian politics'] = ["political debates", "election analysis", "news headlines", "parliament sessions", "interviews with politicians"]; INTEREST_TO_WATCH_MAP_GEN['travel (domestic/international)'] = ["travel vlogs (india/abroad)", "destination guides", "travel tips & hacks", "road trip videos", "hotel/resort reviews"]; INTEREST_TO_WATCH_MAP_GEN['street food safari'] = ["food review vlogs (india)", "street food discovery", "regional recipe tutorials", "restaurant reviews", "cooking shows"]; INTEREST_TO_WATCH_MAP_GEN['yoga & meditation'] = ["yoga tutorials (hindi/english)", "guided meditation", "pranayama techniques", "spiritual discourses", "wellness retreats info"]; INTEREST_TO_WATCH_MAP_GEN['online gaming (bgmi/freefire)'] = ["gaming streams (indian gamers)", "gameplay highlights", "esports tournaments (india)", "new game reviews", "gaming setup tours"]; INTEREST_TO_WATCH_MAP_GEN['fitness & gym'] = ["home workout routines (india focus)", "gym motivation videos", "indian fitness influencers", "healthy indian recipes", "supplement reviews"]; INTEREST_TO_WATCH_MAP_GEN['cooking & recipes (regional/global)'] = ["north indian recipes", "south indian cooking", "bengali sweets tutorials", "maharashtrian thali", "quick snack recipes", "baking tutorials"]; INTEREST_TO_WATCH_MAP_GEN['online shopping (fashion/electronics)'] = ["product unboxing videos", "fashion hauls (myntra/ajio)", "gadget reviews", "sale alert videos", "comparison videos"]; INTEREST_TO_WATCH_MAP_GEN['stock market (dalal street)'] = ["stock analysis videos", "investment strategies", "ipo reviews", "economic news commentary", "trading tutorials"]
ALL_WATCH_CATEGORIES_GEN = list(set([cat for sublist in INTEREST_TO_WATCH_MAP_GEN.values() for cat in sublist] + DEFAULT_WATCH_CATEGORIES_GEN))
WATCH_CATEGORY_OPTIONS = sorted(ALL_WATCH_CATEGORIES_GEN)

# --- Hardware/Embedding Setup ---
DEVICE = torch.device('cpu') # Ensure CPU for broader compatibility on smaller instances
logger.info(f"Application will use device: {DEVICE}")
SBERT_MODEL_NAME = 'all-MiniLM-L6-v2'
sbert_model = None # Initialize globally, will be loaded by load_model_and_data
EMBEDDING_DIM = 384 # Set based on SBERT_MODEL_NAME

# --- Load Artifacts and Model (Global Variables) ---
loaded_artifacts = None
campaigns_df_preprocessed = None
model = None # GNN Model
predictor = None # Link Predictor
graph_node_features = None
graph_edge_index = None

# GNN Model Definitions
class GraphSAGE_GNN(Module):
    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5):
        super().__init__()
        self.conv1 = SAGEConv(in_channels, hidden_channels)
        self.conv2 = SAGEConv(hidden_channels, out_channels)
        self.dropout = Dropout(dropout)
    def forward(self, x, edge_index):
        x = F.relu(self.conv1(x, edge_index))
        x = self.dropout(x)
        x = self.conv2(x, edge_index)
        return x

class LinkPredictor(Module):
    def __init__(self, in_channels):
        super().__init__()
        predictor_hidden_dim = max(32, in_channels // 2) # Ensure hidden dim is reasonable
        self.lin1 = Linear(in_channels * 2, predictor_hidden_dim)
        self.lin2 = Linear(predictor_hidden_dim, 1)
        self.dropout = Dropout(0.3) # Reduced dropout slightly
    def forward(self, x_i, x_j):
        x = torch.cat([x_i, x_j], dim=-1)
        x = F.relu(self.lin1(x))
        x = self.dropout(x)
        x = self.lin2(x)
        return x.squeeze(-1) # Ensure output shape is correct

# === Helper Functions ===
def clear_flask_context_cache():
    gc.collect()
    logger.debug("Performed gc.collect()")

def get_sbert_batch_embeddings(texts, model, batch_size=128):
    """Generates SBERT embeddings for a batch of texts."""
    global EMBEDDING_DIM
    if not model:
        logger.error("SBERT model not loaded!")
        return [np.zeros(EMBEDDING_DIM)] * len(texts)
    if not isinstance(texts, list): texts = list(texts)
    # Handle potential NaN/None/empty strings gracefully
    processed_texts = [str(t).strip() if pd.notna(t) and str(t).strip() else " " for t in texts]
    original_indices = [i for i, t in enumerate(processed_texts) if t != " "] # Indices of non-empty strings
    embeddings = np.zeros((len(texts), EMBEDDING_DIM), dtype=np.float32)
    valid_texts = [processed_texts[i] for i in original_indices]

    if not valid_texts: return list(embeddings) # Return zeros if all inputs were empty/invalid

    try:
        show_bar = False # Disable progress bar for server logs
        with torch.no_grad():
            valid_embeddings = model.encode(valid_texts, convert_to_numpy=True, device=DEVICE, batch_size=batch_size, show_progress_bar=show_bar)

        if valid_embeddings.shape[0] != len(valid_texts):
            logger.warning(f"SBERT encode mismatch! Expected {len(valid_texts)}, got {valid_embeddings.shape[0]}")
            min_len = min(len(original_indices), len(valid_embeddings))
            for i in range(min_len):
                 embeddings[original_indices[i]] = valid_embeddings[i]
        else:
            for i, embed_idx in enumerate(original_indices):
                 embeddings[embed_idx] = valid_embeddings[i]

        return list(embeddings) # Return list of numpy arrays

    except RuntimeError as e: logger.warning(f"Runtime error during SBERT batch encode: {e}. Returning zeros.");
    except Exception as e: logger.warning(f"General error during SBERT batch encode: {e}. Returning zeros.");
    return list(np.zeros((len(texts), EMBEDDING_DIM), dtype=np.float32))

def get_sbert_embedding_pred(text, model):
    """Generates SBERT embedding for a single text."""
    return get_sbert_batch_embeddings([text], model, batch_size=1)[0]

def get_sbert_list_embedding_pred(list_text, model):
    """Generates an average SBERT embedding for a list of text items."""
    global EMBEDDING_DIM
    if not model: return np.zeros(EMBEDDING_DIM)
    if isinstance(list_text, list):
        items = [item.strip() for item in list_text if isinstance(item, str) and item.strip()]
    elif isinstance(list_text, str) and list_text.strip():
        items = [item.strip() for item in list_text.split(',') if item.strip()]
    else: items = []
    if not items: return np.zeros(EMBEDDING_DIM)
    item_embeddings_list = get_sbert_batch_embeddings(items, model, batch_size=len(items))
    item_embeddings = np.array(item_embeddings_list); valid_mask = np.any(item_embeddings != 0, axis=1)
    if not np.any(valid_mask): return np.zeros(EMBEDDING_DIM)
    avg_embedding = np.mean(item_embeddings[valid_mask], axis=0)
    if np.any(~np.isfinite(avg_embedding)):
        logger.warning("Non-finite value encountered in average list embedding. Returning zero vector.")
        return np.zeros(EMBEDDING_DIM)
    return avg_embedding

def standardize_location_pred(loc_str):
    """Standardizes location string using the defined map."""
    return LOCATION_STANDARDIZATION_MAP.get(str(loc_str).lower().strip(), str(loc_str).lower().strip())

def get_location_region_pred(standardized_location):
    """Gets the broader region for a standardized location."""
    for region, locations in LOCATION_REGIONS_MAP.items():
        if standardized_location in locations: return region
    return "Other"

# === Load Model and Data Function ===
def load_model_and_data():
    """Loads SBERT, GNN models, artifacts, and graph data. Downloads if necessary."""
    global loaded_artifacts, campaigns_df_preprocessed, model, predictor, sbert_model
    global graph_node_features, graph_edge_index, EMBEDDING_DIM

    func_name="load_model_and_data"; logger.info(f"[{func_name}] Starting Application Initialization...")

    try:
        # --- Artifact Download/Check Logic ---
        artifacts_dir = PREPROCESSED_DIR
        required_files = [ MODEL_ARTIFACTS_FILE, CAMPAIGNS_PREPROCESSED_FILE, GRAPH_DATA_FILE ]
        full_paths = {f: os.path.join(artifacts_dir, f) for f in required_files}
        artifacts_exist = os.path.exists(artifacts_dir) and \
                          all(os.path.exists(p) for p in full_paths.values())

        if not artifacts_exist:
            logger.warning(f"[{func_name}] Artifacts not found locally in '{artifacts_dir}'. Attempting download...")
            artifact_zip_url = os.environ.get('ARTIFACT_URL') # Expects direct URL (like GitHub Release)
            if not artifact_zip_url:
                logger.error(f"[{func_name}] ARTIFACT_URL environment variable not set. Cannot download artifacts.")
                raise SystemExit("Critical Error: ARTIFACT_URL not set.")

            zip_path = "artifacts_temp.zip"
            download_success = False
            try:
                # <<< SIMPLE DOWNLOAD LOGIC FOR DIRECT URLS (GitHub) >>>
                logger.info(f"[{func_name}] Downloading from direct URL: {artifact_zip_url}...")
                response = requests.get(artifact_zip_url, stream=True, timeout=300) # 5 min timeout
                response.raise_for_status() # Check for HTTP errors (e.g., 404)

                logger.info(f"[{func_name}] Streaming download content to {zip_path}...")
                with open(zip_path, "wb") as f:
                    total_downloaded = 0
                    for chunk in response.iter_content(chunk_size=8192*4):
                        f.write(chunk)
                        total_downloaded += len(chunk)
                logger.info(f"[{func_name}] Download complete: {zip_path} ({total_downloaded / (1024*1024):.2f} MB)")
                download_success = True
                # <<< END SIMPLE DOWNLOAD LOGIC >>>

            except requests.exceptions.Timeout:
                 logger.error(f"[{func_name}] Timeout occurred while downloading artifacts from {artifact_zip_url}.")
                 if os.path.exists(zip_path): os.remove(zip_path)
                 raise SystemExit("Artifact download timed out.") from None
            except requests.exceptions.RequestException as e:
                logger.error(f"[{func_name}] Failed to download artifacts: {e}")
                status_code = e.response.status_code if e.response is not None else "N/A"
                logger.error(f"[{func_name}] Status Code: {status_code}")
                if os.path.exists(zip_path): os.remove(zip_path)
                raise SystemExit(f"Failed to download artifacts (status: {status_code}): {e}") from e

            # --- Extraction Logic ---
            if download_success:
                extract_to_dir = BASE_DIR
                try:
                    logger.info(f"[{func_name}] Extracting {zip_path} to '{extract_to_dir}'...")
                    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                        zip_ref.extractall(extract_to_dir)
                    logger.info(f"[{func_name}] Extraction complete. Artifacts should now be in '{artifacts_dir}'.")
                    artifacts_exist = os.path.exists(artifacts_dir) and \
                                      all(os.path.exists(p) for p in full_paths.values())
                    if not artifacts_exist:
                         logger.error(f"[{func_name}] Artifacts still not found after extraction. Check ZIP contents/structure.")
                         if os.path.exists(artifacts_dir): logger.error(f"Files found in {artifacts_dir}: {os.listdir(artifacts_dir)}")
                         else: logger.error(f"Directory {artifacts_dir} does not exist after extraction.")
                         raise SystemExit("Artifact extraction failed or ZIP structure incorrect.")
                except zipfile.BadZipFile:
                     logger.error(f"[{func_name}] Error: Downloaded file '{zip_path}' is not a valid ZIP file.")
                     raise SystemExit("Downloaded artifact file is corrupted or not a ZIP.")
                except Exception as e:
                     logger.error(f"[{func_name}] Failed to extract artifacts: {e}", exc_info=True)
                     raise SystemExit(f"Failed to extract artifacts: {e}") from e
                finally:
                    if os.path.exists(zip_path):
                        try:
                            os.remove(zip_path)
                            logger.info(f"[{func_name}] Removed temporary file: {zip_path}")
                        except OSError as e:
                            logger.warning(f"[{func_name}] Could not remove temporary zip file {zip_path}: {e}")
        else:
             logger.info(f"[{func_name}] Found existing artifacts in '{artifacts_dir}'. Skipping download.")
        # --- <<< End Artifact Handling Logic >>> ---

        # --- Load Models and Data ---
        if sbert_model is None:
            logger.info(f"[{func_name}] Loading SBERT model ({SBERT_MODEL_NAME})...")
            start_sbert = time.time()
            sbert_model = SentenceTransformer(SBERT_MODEL_NAME, device=DEVICE)
            logger.info(f"[{func_name}] SBERT model loaded in {time.time() - start_sbert:.2f}s.")
        else: logger.info(f"[{func_name}] SBERT model already loaded.")

        logger.info(f"[{func_name}] Loading main artifacts from {full_paths[MODEL_ARTIFACTS_FILE]}...")
        with open(full_paths[MODEL_ARTIFACTS_FILE], 'rb') as f:
            # <<< FIX: Add map_location for loading CUDA objects on CPU >>>
            loaded_artifacts = joblib.load(f, mmap_mode=None, allow_pickle=True, encoding='latin1', map_location=torch.device('cpu'))
        logger.info(f"[{func_name}] Loaded artifacts keys: {list(loaded_artifacts.keys())}")

        logger.info(f"[{func_name}] Loading preprocessed campaigns from {full_paths[CAMPAIGNS_PREPROCESSED_FILE]}...")
        campaigns_df_preprocessed = pd.read_csv(full_paths[CAMPAIGNS_PREPROCESSED_FILE], index_col=CAMPAIGN_ID_COL)
        logger.info(f"[{func_name}] Loaded {len(campaigns_df_preprocessed)} campaigns.")

        logger.info(f"[{func_name}] Loading graph data from {full_paths[GRAPH_DATA_FILE]}...")
        # This already had map_location=DEVICE, which is good since DEVICE is 'cpu'
        graph_data = torch.load(full_paths[GRAPH_DATA_FILE], map_location=DEVICE)
        if isinstance(graph_data, Data):
            graph_node_features = graph_data.x; graph_edge_index = graph_data.edge_index
            logger.info(f"[{func_name}] Loaded graph data: Nodes={graph_node_features.shape[0]}, Edges={graph_edge_index.shape[1]}")
        elif isinstance(graph_data, dict) and 'x' in graph_data and 'edge_index' in graph_data:
            graph_node_features = graph_data['x'].to(DEVICE); graph_edge_index = graph_data['edge_index'].to(DEVICE)
            logger.info(f"[{func_name}] Loaded graph data from dict: Nodes={graph_node_features.shape[0]}, Edges={graph_edge_index.shape[1]}")
        else: raise TypeError(f"Unsupported graph data format in {full_paths[GRAPH_DATA_FILE]}.")

        logger.info(f"[{func_name}] Initializing GNN model and Link Predictor...")
        num_node_features = graph_node_features.shape[1]
        gnn_hidden_channels = loaded_artifacts.get('gnn_hidden_channels', 128)
        gnn_output_channels = loaded_artifacts.get('gnn_output_channels', 64)
        model = GraphSAGE_GNN(num_node_features, gnn_hidden_channels, gnn_output_channels)
        predictor = LinkPredictor(gnn_output_channels)
        # Ensure model state dicts are loaded onto the correct device (CPU)
        model.load_state_dict(loaded_artifacts['model_state_dict'], strict=False) # strict=False might be needed if layer names changed slightly
        predictor.load_state_dict(loaded_artifacts['predictor_state_dict'], strict=False) # strict=False might be needed
        model.to(DEVICE); predictor.to(DEVICE); model.eval(); predictor.eval()
        logger.info(f"[{func_name}] GNN model and predictor initialized and loaded.")

        if sbert_model:
            try:
                actual_sbert_dim = sbert_model.get_sentence_embedding_dimension()
                if actual_sbert_dim != EMBEDDING_DIM:
                    logger.warning(f"[{func_name}] Configured EMBEDDING_DIM ({EMBEDDING_DIM}) != SBERT model ({actual_sbert_dim}). Updating EMBEDDING_DIM.")
                    EMBEDDING_DIM = actual_sbert_dim
            except Exception as e: logger.warning(f"[{func_name}] Could not verify SBERT model dimension: {e}")
        logger.info(f"[{func_name}] Application Initialization Complete.")

    except FileNotFoundError as fnf_e:
         logger.error(f"[{func_name}] ERROR: Required file not found after check/download attempts: {fnf_e.filename}", exc_info=True)
         raise SystemExit(f"Critical file missing: {fnf_e.filename}") from fnf_e
    except SystemExit: raise # Re-raise SystemExit
    except Exception as e:
         logger.error(f"[{func_name}] Unexpected error during application initialization: {e}", exc_info=True)
         raise SystemExit(f"Failed to initialize application resources: {e}") from e

# === <<< CALL THE LOADING FUNCTION IN GLOBAL SCOPE >>> ===
logger.info("Starting application initialization process (global scope)...")
try:
    load_model_and_data() # Load everything
    logger.info("Initialization process finished.")
except SystemExit as se:
    logger.critical(f"Application initialization aborted by SystemExit: {se}")
except Exception as e:
     logger.critical(f"CRITICAL FAILURE during global initialization: {e}", exc_info=True)

# === Prediction Function ===
@torch.no_grad()
def predict_campaigns_gnn_flask(user_data: dict):
    """Performs campaign prediction based on user data using loaded models."""
    global loaded_artifacts, campaigns_df_preprocessed, model, predictor, sbert_model, graph_node_features, graph_edge_index
    func_name = "predict_campaigns_gnn_flask"; user_display_id = user_data.get(USER_ID_COL, 'N/A')
    logger.info(f"[{func_name}] Received prediction request for User: {user_display_id}")

    # --- Pre-check: Ensure resources are loaded ---
    if not all([loaded_artifacts, campaigns_df_preprocessed is not None, model, predictor, sbert_model, graph_node_features is not None, graph_edge_index is not None]):
        logger.error(f"[{func_name}] Critical server resources not ready/loaded. Aborting prediction.")
        return [], 0.0, "Error: Server resources are not ready. Please try again later or contact support."

    start_total_time = time.time()
    try:
        # --- 1. User Data Preprocessing ---
        user_df = pd.DataFrame([user_data])
        user_defaults_pred_scalar = {USER_AGE_COL: 'unknown', USER_LOCATION_COL: 'unknown', USER_ACTIVITY_COL: 'medium (weekly)', USER_MONETARY_COL: 'medium spender', USER_DEVICE_COL: 'android smartphone (mid-range)', USER_WEATHER_COL: 'clear skies'}
        for col, default in user_df.columns: user_df[col] = default
        user_df.fillna(user_defaults_pred_scalar, inplace=True)
        for col in [USER_INTERESTS_COL, USER_WATCH_CATEGORIES_COL]:
            if col not in user_df.columns: user_df[col] = pd.Series([[] for _ in range(len(user_df))], index=user_df.index)
            else: user_df[col] = user_df[col].apply(lambda x: x if isinstance(x, list) else ([] if pd.isna(x) else [str(x)]))
        for col in [USER_LOCATION_COL, USER_ACTIVITY_COL, USER_MONETARY_COL, USER_DEVICE_COL, USER_WEATHER_COL]:
             user_df[col] = user_df[col].astype(str).str.lower().str.strip()
        user_loc_raw = user_df[USER_LOCATION_COL].iloc[0]; user_loc_standardized = standardize_location_pred(user_loc_raw); user_region = get_location_region_pred(user_loc_standardized)
        user_activity_level = user_df[USER_ACTIVITY_COL].iloc[0]; user_interest_data = user_df[USER_INTERESTS_COL].iloc[0]; user_watch_data = user_df[USER_WATCH_CATEGORIES_COL].iloc[0]
        logger.info(f"[{func_name}] User Loc:'{user_loc_raw}'->Std:'{user_loc_standardized}'->Region:'{user_region}'. Activity:'{user_activity_level}'")

        # --- 2. Calculate GNN Embeddings ---
        logger.info(f"[{func_name}] Calculating GNN node embeddings..."); start_time_gnn = time.time()
        x_feat = graph_node_features.to(DEVICE).contiguous(); edge_idx = graph_edge_index.to(DEVICE).contiguous()
        all_node_embeddings = model(x_feat, edge_idx)
        if id(x_feat) != id(graph_node_features): del x_feat
        if id(edge_idx) != id(graph_edge_index): del edge_idx
        gc.collect(); end_time_gnn = time.time()
        logger.info(f"[{func_name}] GNN embedding calculation took: {end_time_gnn - start_time_gnn:.3f}s")

        # --- 3. Calculate GNN Scores & Avg Relevance ---
        num_users = loaded_artifacts['num_users']; num_campaigns = loaded_artifacts['num_campaigns']
        user_map = loaded_artifacts['user_map']; campaign_indices = list(range(num_users, num_users + num_campaigns))
        if not campaign_indices or max(campaign_indices) >= all_node_embeddings.shape[0]:
            raise ValueError(f"Campaign indices out of bounds ({max(campaign_indices)}) for node embeddings ({all_node_embeddings.shape[0]}).")
        user_orig_id = user_df.iloc[0].get(USER_ID_COL); user_node_idx = user_map.get(str(user_orig_id)) if user_orig_id else None
        if user_node_idx is not None and user_node_idx < num_users:
            user_embedding = all_node_embeddings[user_node_idx].unsqueeze(0); logger.info(f"[{func_name}] Found known user '{user_orig_id}' (Idx: {user_node_idx}).")
        else:
            logger.warning(f"[{func_name}] User '{user_orig_id}' unknown. Using average user embedding.");
            user_embedding = all_node_embeddings[:num_users].mean(dim=0, keepdim=True) if num_users > 0 else torch.zeros((1, all_node_embeddings.shape[1]), device=DEVICE)
        campaign_embeddings_gnn = all_node_embeddings[campaign_indices]; user_embedding_repeated = user_embedding.repeat(len(campaign_indices), 1);
        all_scores = predictor(user_embedding_repeated, campaign_embeddings_gnn); all_scores_sigmoid = torch.sigmoid(all_scores); all_scores_np = all_scores_sigmoid.cpu().numpy()
        avg_gnn_score_threshold = 0.5; relevant_scores = all_scores_np[all_scores_np > avg_gnn_score_threshold]
        avg_relevance_score = np.mean(relevant_scores) if relevant_scores.size > 0 else (np.mean(all_scores_np) if all_scores_np.size > 0 else 0.0)
        logger.info(f"[{func_name}] User Avg GNN Relevance Score (>{avg_gnn_score_threshold:.2f}): {avg_relevance_score:.4f}")

        # --- 4. Calculate Dynamic User Tier ---
        high_activity_keywords = ['very high', 'daily', 'high', 'multiple weekly']; medium_activity_keywords = ['medium', 'weekly']
        score_threshold_elite = 0.75; score_threshold_prem = 0.60; dynamic_user_tier = "Basic"
        is_high_activity = any(keyword in user_activity_level for keyword in high_activity_keywords); is_medium_activity = any(keyword in user_activity_level for keyword in medium_activity_keywords)
        if is_high_activity and avg_relevance_score >= score_threshold_prem: dynamic_user_tier = "Elite"
        elif (is_medium_activity and avg_relevance_score >= score_threshold_elite) or (avg_relevance_score >= score_threshold_elite): dynamic_user_tier = "Premium"
        logger.info(f"[{func_name}] Calculated dynamic user tier: {dynamic_user_tier}")

        # --- 5. Get User SBERT Embeddings ---
        logger.debug(f"[{func_name}] Generating SBERT embeddings for user profile..."); start_time_sbert_user = time.time()
        user_interest_emb_np = get_sbert_list_embedding_pred(user_interest_data, sbert_model).reshape(1, -1)
        user_watch_emb_np = get_sbert_list_embedding_pred(user_watch_data, sbert_model).reshape(1, -1)
        end_time_sbert_user = time.time(); logger.info(f"[{func_name}] User SBERT calculation took: {end_time_sbert_user - start_time_sbert_user:.3f}s")

        # --- 6. Prepare Candidate List (SBERT & Similarities) ---
        logger.info(f"[{func_name}] Calculating proximity & semantic scores for {num_campaigns} campaigns...");
        reverse_campaign_map = loaded_artifacts['reverse_campaign_map']
        campaign_data_subset = campaigns_df_preprocessed[[CAMPAIGN_LOCATION_COL, CAMPAIGN_PROMO_COL, CAMPAIGN_CATEGORY_COL]].copy()
        campaign_data_subset[CAMPAIGN_LOCATION_COL] = campaign_data_subset[CAMPAIGN_LOCATION_COL].astype(str).str.lower().str.strip().map(standardize_location_pred)
        campaign_ids_in_order = []; promo_texts_to_embed = []; category_texts_to_embed = []; gnn_scores_in_order = {}
        for i, gnn_score_val in enumerate(all_scores_np):
             campaign_node_idx = campaign_indices[i]; original_campaign_id = reverse_campaign_map.get(campaign_node_idx)
             if not original_campaign_id or original_campaign_id not in campaign_data_subset.index: continue
             campaign_ids_in_order.append(original_campaign_id); gnn_scores_in_order[original_campaign_id] = float(gnn_score_val)
             promo_texts_to_embed.append(campaign_data_subset.loc[original_campaign_id, CAMPAIGN_PROMO_COL])
             category_texts_to_embed.append(campaign_data_subset.loc[original_campaign_id, CAMPAIGN_CATEGORY_COL])
        logger.info(f"[{func_name}] Calculating batch SBERT embeddings for {len(campaign_ids_in_order)} valid campaigns..."); start_time_sbert_camp = time.time()
        campaign_promo_embeds_list = get_sbert_batch_embeddings(promo_texts_to_embed, sbert_model)
        campaign_category_embeds_list = get_sbert_batch_embeddings(category_texts_to_embed, sbert_model)
        end_time_sbert_camp = time.time(); logger.info(f"[{func_name}] Campaign SBERT calculation took: {end_time_sbert_camp - start_time_sbert_camp:.3f}s")
        campaign_promo_embeddings = {cid: emb for cid, emb in zip(campaign_ids_in_order, campaign_promo_embeds_list)}
        campaign_category_embeddings = {cid: emb for cid, emb in zip(campaign_ids_in_order, campaign_category_embeds_list)}
        all_candidates_info = []
        for original_campaign_id in campaign_ids_in_order:
            gnn_score = gnn_scores_in_order[original_campaign_id]; campaign_loc_std = campaign_data_subset.loc[original_campaign_id, CAMPAIGN_LOCATION_COL]; campaign_region = get_location_region_pred(campaign_loc_std)
            proximity_score = 2 if campaign_loc_std == user_loc_standardized else (1 if campaign_region != "Other" and campaign_region != "Unknown Region" and campaign_region == user_region else 0)
            interest_promo_sim = 0.0; watch_cat_sim = 0.0
            camp_promo_embed = campaign_promo_embeddings.get(original_campaign_id); camp_cat_embed = campaign_category_embeddings.get(original_campaign_id)
            try:
                if user_interest_emb_np.size > 1 and camp_promo_embed is not None and camp_promo_embed.size > 1 and user_interest_emb_np.shape[1] == camp_promo_embed.shape[0]: interest_promo_sim = cosine_similarity(user_interest_emb_np, camp_promo_embed.reshape(1, -1))[0][0]
                if user_watch_emb_np.size > 1 and camp_cat_embed is not None and camp_cat_embed.size > 1 and user_watch_emb_np.shape[1] == camp_cat_embed.shape[0]: watch_cat_sim = cosine_similarity(user_watch_emb_np, camp_cat_embed.reshape(1, -1))[0][0]
            except Exception as sim_e: logger.warning(f"[{func_name}] Sim calc error for {original_campaign_id}: {sim_e}")
            all_candidates_info.append({'campaign_id': original_campaign_id, 'gnn_score': gnn_score, 'interest_promo_sim': max(0, float(interest_promo_sim)), 'watch_cat_sim': max(0, float(watch_cat_sim)), 'location': campaign_loc_std, 'proximity': proximity_score })

        # --- 7. Rank Candidates ---
        logger.info(f"[{func_name}] Ranking {len(all_candidates_info)} candidates..."); exact_matches = []; region_matches = []; other_matches = []
        for cand in all_candidates_info:
            if cand['proximity'] == 2: exact_matches.append(cand)
            elif cand['proximity'] == 1: region_matches.append(cand)
            else: other_matches.append(cand)
        ranking_key = lambda x: (x['watch_cat_sim'], x['interest_promo_sim'], x['gnn_score'])
        exact_matches.sort(key=ranking_key, reverse=True); region_matches.sort(key=ranking_key, reverse=True); other_matches.sort(key=ranking_key, reverse=True)
        logger.debug(f"Ranked {len(exact_matches)} exact, {len(region_matches)} region, {len(other_matches)} other matches.")

        # --- 8. Build Final List ---
        final_recommendations_candidates = []; processed_ids = set(); num_recommendations = 10
        final_recommendations_candidates.extend([c for c in exact_matches if c['campaign_id'] not in processed_ids and not processed_ids.add(c['campaign_id'])][:num_recommendations])
        if len(final_recommendations_candidates) < num_recommendations:
             final_recommendations_candidates.extend([c for c in region_matches if c['campaign_id'] not in processed_ids and not processed_ids.add(c['campaign_id'])][:num_recommendations - len(final_recommendations_candidates)])
        if len(final_recommendations_candidates) < num_recommendations:
             final_recommendations_candidates.extend([c for c in other_matches if c['campaign_id'] not in processed_ids and not processed_ids.add(c['campaign_id'])][:num_recommendations - len(final_recommendations_candidates)])
        logger.info(f"Final recommendation list size: {len(final_recommendations_candidates)}")

        # --- 9. Format Output & Calculate Satisfaction ---
        logger.info(f"[{func_name}] Formatting final output..."); results_list = []; total_weighted_score = 0
        proximity_satisfaction_weights = {2: 1.0, 1: 0.75, 0: 0.5}; SIM_THRESHOLD = 0.25; HIGH_GNN_THRESHOLD = 0.7
        for rank, candidate in enumerate(final_recommendations_candidates):
            original_campaign_id = candidate['campaign_id']
            try:
                campaign_info_series = campaigns_df_preprocessed.loc[original_campaign_id]; display_score = candidate['gnn_score']
                reason_parts = []; proximity_level = candidate['proximity']; watch_sim = candidate['watch_cat_sim']; interest_sim = candidate['interest_promo_sim']; gnn_score_val = candidate['gnn_score']
                if proximity_level == 2: reason_parts.append("Exact Location Match")
                elif proximity_level == 1: reason_parts.append("Nearby Region Match")
                is_watch_strong = watch_sim >= SIM_THRESHOLD; is_interest_strong = interest_sim >= SIM_THRESHOLD
                if is_watch_strong: reason_parts.append(f"Matches Watch History ({watch_sim:.2f})")
                if is_interest_strong and (not is_watch_strong or interest_sim > watch_sim): reason_parts.append(f"Matches Interests ({interest_sim:.2f})")
                elif is_interest_strong: reason_parts.append(f"Interest Match ({interest_sim:.2f})")
                if not is_watch_strong and not is_interest_strong:
                    if gnn_score_val >= HIGH_GNN_THRESHOLD: reason_parts.append(f"High Relevance Score ({gnn_score_val:.2f})")
                    else: reason_parts.append(f"Good Overall Match ({gnn_score_val:.2f})")
                elif gnn_score_val >= HIGH_GNN_THRESHOLD + 0.1: reason_parts.append(f"Very High Relevance ({gnn_score_val:.2f})")
                reason = " + ".join(reason_parts) if reason_parts else "General Recommendation"
                results_list.append({"campaign_id": original_campaign_id, "business_name": campaign_info_series.get(CAMPAIGN_BUSINESS_NAME_COL, "N/A"), "promo": campaign_info_series.get(CAMPAIGN_PROMO_COL, "N/A"), "category": campaign_info_series.get(CAMPAIGN_CATEGORY_COL, "N/A"), "location": candidate['location'].title(), "proximity_level": proximity_level, "score": round(display_score, 4), "tier": dynamic_user_tier, "interest_promo_sim": round(interest_sim, 4), "watch_cat_sim": round(watch_sim, 4), "reason": reason })
                total_weighted_score += display_score * proximity_satisfaction_weights.get(proximity_level, 0.1)
            except KeyError: logger.warning(f"[{func_name}] Campaign ID {original_campaign_id} missing during formatting. Skipping."); continue
            except Exception as e: logger.warning(f"[{func_name}] Error formatting final rec {original_campaign_id}: {e}", exc_info=True); continue
        satisfaction_overall = (total_weighted_score / len(results_list)) * 100 if results_list else 0.0
        num_exact = sum(1 for r in results_list if r['proximity_level'] == 2); num_region = sum(1 for r in results_list if r['proximity_level'] == 1)
        logger.info(f"[{func_name}] Prediction successful for User: {user_display_id}. Recs: {len(results_list)} (Exact: {num_exact}, Region: {num_region}). Est. Satisfaction: {satisfaction_overall:.2f}%.")

        # --- Cleanup ---
        if 'all_node_embeddings' in locals(): del all_node_embeddings;
        if 'user_embedding' in locals(): del user_embedding;
        if 'campaign_embeddings_gnn' in locals(): del campaign_embeddings_gnn;
        if 'all_scores' in locals(): del all_scores;
        if 'all_scores_sigmoid' in locals(): del all_scores_sigmoid;
        if 'campaign_promo_embeddings' in locals(): del campaign_promo_embeddings;
        if 'campaign_category_embeddings' in locals(): del campaign_category_embeddings;
        if 'user_interest_emb_np' in locals(): del user_interest_emb_np;
        if 'user_watch_emb_np' in locals(): del user_watch_emb_np;
        gc.collect()

        end_total_time = time.time()
        logger.info(f"[{func_name}] Total prediction request time for User {user_display_id}: {end_total_time - start_total_time:.3f}s")
        return results_list, satisfaction_overall, None

    except Exception as e:
        logger.error(f"[{func_name}] UNEXPECTED ERROR during prediction for User {user_display_id}: {e}", exc_info=True)
        if 'all_node_embeddings' in locals(): del all_node_embeddings
        if 'campaign_promo_embeddings' in locals(): del campaign_promo_embeddings
        if 'campaign_category_embeddings' in locals(): del campaign_category_embeddings
        gc.collect()
        return [], 0.0, f'An internal error occurred during prediction: {type(e).__name__}. Please try again.'
    finally:
        clear_flask_context_cache()

# === Flask Routes ===
def _get_dropdown_options():
    """Helper to populate dropdown options for the form, using loaded artifacts or defaults."""
    options = {'common_interests': INTERESTS_OPTIONS, 'common_watch_categories': WATCH_CATEGORY_OPTIONS, 'locations': ['Enter Manually'], 'age_groups': ["16-22 (Student)", "23-30 (Young Pro)", "31-40 (Settling Down)", "41-55 (Established)", "56-65 (Senior)", "65+ (Retired)"], 'device_types': ["Android Smartphone (Mid-Range)", "Android Smartphone (High-End)", "iOS Smartphone", "Tablet (Android)", "Tablet (iOS)", "Windows Desktop/Laptop", "MacOS Desktop/Laptop"], 'activity_levels': ["Very High (Daily+)", "High (Multiple Weekly)", "Medium (Weekly)", "Low (Monthly)", "Very Low (Infrequent)"], 'monetary_levels': ["Low Spender", "Medium Spender", "High Spender", "Very High Spender"], 'weather_options': ["Scorching Heat", "Humid & Sticky", "Pleasant Breeze", "Monsoon Downpour", "Winter Fog (North India)", "Dry Heat (Rajasthan)", "Coastal Humidity", "Hill Station Chill", "Clear Skies", "Overcast"]}
    error_msg = None
    try:
        if loaded_artifacts is None: raise RuntimeError("Model artifacts not loaded.")
        location_encoder = loaded_artifacts.get('location_encoder')
        if location_encoder and hasattr(location_encoder, 'classes_'): options['locations'] = sorted([loc.title() for loc in location_encoder.classes_ if loc != 'unknown'])
        encoder_map = {f'{USER_AGE_COL}_encoder': 'age_groups', f'{USER_DEVICE_COL}_encoder': 'device_types', f'{USER_ACTIVITY_COL}_encoder': 'activity_levels', f'{USER_MONETARY_COL}_encoder': 'monetary_levels', f'{USER_WEATHER_COL}_encoder': 'weather_options'}
        for artifact_key, option_key in encoder_map.items():
             encoder = loaded_artifacts.get(artifact_key)
             if encoder and hasattr(encoder, 'classes_'): options[option_key] = [opt.title() for opt in encoder.classes_]
             else: logger.warning(f"Encoder '{artifact_key}' not found/invalid. Using default options for '{option_key}'.")
    except Exception as e:
        logger.error(f"Error populating dropdown options: {e}", exc_info=True)
        error_msg = "Error loading some dropdown options. Using default values."
    default_keys = list(options.keys())
    for key in default_keys:
        if key not in options or not options[key]: logger.warning(f"Dropdown options for '{key}' empty.")
    return options, error_msg

@app.route('/')
def index():
    """Renders the main page with the input form."""
    options, error_msg = _get_dropdown_options()
    return render_template('index.html', results=None, error=error_msg, submitted=False, options=options)

@app.route('/predict', methods=['POST'])
def predict():
    """Handles form submission, calls prediction function, and renders results."""
    error_msg = None; recommendations = []; satisfaction=0.0
    options, error_msg_opts = _get_dropdown_options()
    submitted_data_for_template = request.form.to_dict(flat=False)
    if error_msg_opts:
        logger.warning("Dropdown options failed to load, returning error on predict request.")
        return render_template('index.html', results=[], error=error_msg_opts, submitted=True, satisfaction=0, options=options, submitted_data=submitted_data_for_template)
    try:
        user_data = {}; form_ok = True
        required_fields = {USER_LOCATION_COL: request.form.get('location'), USER_ACTIVITY_COL: request.form.get('activity_level')}
        for key, value in required_fields.items():
            if not value or value.strip() == '': error_msg = f"Please provide a value for '{key.replace('_',' ').title()}'."; form_ok = False; break
            user_data[key] = value.strip()
        if form_ok:
            user_data[USER_ID_COL] = request.form.get('user_id', f"ANON_{random.randint(1000,9999)}").strip()
            user_data[USER_AGE_COL] = request.form.get('age_group', 'unknown')
            user_data[USER_DEVICE_COL] = request.form.get('device_type', 'unknown')
            user_data[USER_MONETARY_COL] = request.form.get('monetary_level', 'unknown')
            user_data[USER_WEATHER_COL] = request.form.get('simulated_weather', 'unknown')
            interests_list = request.form.getlist(USER_INTERESTS_COL)
            watch_list = request.form.getlist(USER_WATCH_CATEGORIES_COL)
            user_data[USER_INTERESTS_COL] = interests_list if interests_list else []
            user_data[USER_WATCH_CATEGORIES_COL] = watch_list if watch_list else []
            log_data = {k:v for k,v in user_data.items() if k not in [USER_INTERESTS_COL, USER_WATCH_CATEGORIES_COL]}
            log_data[f"{USER_INTERESTS_COL}_count"] = len(user_data[USER_INTERESTS_COL])
            log_data[f"{USER_WATCH_CATEGORIES_COL}_count"] = len(user_data[USER_WATCH_CATEGORIES_COL])
            logger.info(f"Received prediction request for user data: {log_data}")
            recommendations, satisfaction, pred_error_msg = predict_campaigns_gnn_flask(user_data)
            if pred_error_msg: error_msg = pred_error_msg
        elif not error_msg: error_msg = "Form validation failed. Please check required fields."
    except Exception as e:
        logger.error(f"Error processing /predict request: {e}", exc_info=True)
        error_msg = f"An unexpected server error occurred while processing your request."
        recommendations = []; satisfaction = 0.0
    return render_template('index.html', results=recommendations, error=error_msg, submitted=True, satisfaction=satisfaction, submitted_data=submitted_data_for_template, options=options)

# === Main Execution (Only for running locally with `python app.py`) ===
if __name__ == "__main__":
    logger.info("Starting Flask DEVELOPMENT server (via __main__)...")
    if model is None or predictor is None or loaded_artifacts is None:
         logger.warning("!!! Models/artifacts may not have loaded correctly. Check logs. !!!")
    is_debug = os.environ.get("FLASK_DEBUG", "False").lower() in ("true", "1", "t")
    port = int(os.environ.get("PORT", 5000))
    use_threading = not is_debug
    logger.info(f"Running development server on http://0.0.0.0:{port}/ | Debug: {is_debug} | Threaded: {use_threading}")
    app.run(host='0.0.0.0', port=port, debug=is_debug, threaded=use_threading)
